{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [0.2, 0.1],\n",
    "    [0.4, 0.2],\n",
    "    [-0.7, -0.6],\n",
    "    [-0.2, 0.5],\n",
    "    [0.5, -0.7]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array([\n",
    "    [2,3,5],\n",
    "    [6,5,1],\n",
    "    [-5,7,3],\n",
    "    [-1,3,9],\n",
    "    [1,2,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__():\n",
    "        self.w = np.random()\n",
    "        self.b = np.random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "])\n",
    "\n",
    "label_1 = np.array([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "])\n",
    "\n",
    "label_2 = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "label_3 = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "label_4 = np.array([\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "])\n",
    "\n",
    "label_5 = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pattern_0 = label_0.flatten() #(5x5 -> 25x1)\n",
    "\n",
    "y_0=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_pattern_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot code / 타겟 = 1, 나머지 = 0\n",
    "label_one_hot=np.zeros(10)\n",
    "label_one_hot[y_0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size = 25x1 (flatten)\n",
    "# output size = 10x1\n",
    "input_size=25\n",
    "output_size=10\n",
    "\n",
    "w=np.random.random(input_size)\n",
    "b=np.random.random()\n",
    "\n",
    "# w 전치행렬로 만든 후 행렬 곱 연산\n",
    "def perceptron(x, w, b):\n",
    "    weighted_sum = np.dot(x, w) + b\n",
    "    activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = perceptron(input_pattern_0, w,b)\n",
    "print(f\"Output, {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (X): (5, 25)\n",
      "Output shape (y): (5, 5)\n",
      "Output for sample 0: [[0.14498756 0.112302   0.24668721 0.33645497 0.15956827]\n",
      " [0.04076317 0.06350774 0.53724803 0.29726253 0.06121853]\n",
      " [0.03828966 0.14115689 0.45083258 0.26931452 0.10040635]\n",
      " [0.02832849 0.39673906 0.43786409 0.06771374 0.06935462]\n",
      " [0.05546969 0.09530139 0.47318398 0.28528454 0.09076041]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the labels (5x5 arrays)\n",
    "labels = np.array([\n",
    "    [\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Corresponding labels (0-5)\n",
    "targets = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Prepare the input and output data\n",
    "X = np.array([label.flatten() for label in labels])  # Flatten each label (5x5 -> 25x1)\n",
    "y = np.zeros((len(targets), 5))  # One-hot encode the targets\n",
    "for i, target in enumerate(targets):\n",
    "    y[i, target-1] = 1\n",
    "\n",
    "print(\"Input shape (X):\", X.shape)  # Should be (5, 25)\n",
    "print(\"Output shape (y):\", y.shape)  # Should be (5, 5)\n",
    "\n",
    "# Example perceptron model\n",
    "input_size = X.shape[1]  # 25\n",
    "output_size = y.shape[1]  # 5\n",
    "\n",
    "# (5,25) x (25,5) = (5,5)\n",
    "# Initialize weights and bias\n",
    "w = np.random.random((input_size, output_size))  # Shape: (5, 25)\n",
    "b = np.random.random(output_size)  # Shape: (5,)\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    \"\"\"\n",
    "    Perceptron function for multi-class classification.\n",
    "    \"\"\"\n",
    "    weighted_sum = x @ w + b  # Shape: (5,)\n",
    "    return weighted_sum\n",
    "\n",
    "def softmax(a): # (b, n)\n",
    "    y = np.exp(a) / np.sum(np.exp(a), axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "output = softmax(perceptron(X,w,b))\n",
    "print(\"Output for sample 0:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X, w, b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = output\n",
    "# t = target\n",
    "def mse_loss(y, t):\n",
    "    return np.mean((y - t) ** 2)\n",
    "\n",
    "def mse_loss_gradient(y, t):\n",
    "    return (2 / y.shape[0]) * (y - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse_loss(softmax(perceptron(X,w,b)),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01 # 가우시안 정규 분포로 랜덤하게 초기화 (초기화 방법 수정해야함)\n",
    "        self.bias = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        grad_weights = self.input.T @ loss_grad\n",
    "        grad_bais = np.sum(loss_grad, axis=0)\n",
    "\n",
    "        grad_input = loss_grad @ self.weights.T\n",
    "\n",
    "        self.weights -= self.learning_rate * grad_weights\n",
    "        self.bias -= self.learning_rate * grad_bais\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dx_out):\n",
    "        return dx_out\n",
    "\n",
    "class CategoricalCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        return -np.sum(y_true*np.log(y_pred))\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.y_pred - self.y_true\n",
    "\n",
    "    \n",
    "class Mse_loss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean((y_pred-y_true)**2)\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.y_pred.shape[0]\n",
    "        return (2 / batch_size) * (self.y_pred-self.y_true)\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "            outputs.append(x)\n",
    "        return outputs[-1] # forward 진행 시 outputs 리스트에 append하게 되는데 마지막 레이어의 output 값을 return 하기 위해 outputs[-1]을 return 한다다\n",
    "\n",
    "    def backward(self, loss_grad: np.ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers): # 레이어의 역순으로 grad값을 업데이트한다다\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = Sequential([\n",
    "    Dense(25, 5),\n",
    "    Softmax()\n",
    "])\n",
    "loss=CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss : 8.013184966086884\n",
      "Epoch 10, Loss : 6.58757986910486\n",
      "Epoch 20, Loss : 5.581384932541907\n",
      "Epoch 30, Loss : 4.806026232710713\n",
      "Epoch 40, Loss : 4.204102815168264\n",
      "Epoch 50, Loss : 3.731139726283908\n",
      "Epoch 60, Loss : 3.352615936148978\n",
      "Epoch 70, Loss : 3.043581637651612\n",
      "Epoch 80, Loss : 2.7865253523907465\n",
      "Epoch 90, Loss : 2.569162156648956\n",
      "Epoch 100, Loss : 2.3827553167290314\n",
      "Epoch 110, Loss : 2.2209697569367437\n",
      "Epoch 120, Loss : 2.0791147253891884\n",
      "Epoch 130, Loss : 1.953645296756675\n",
      "Epoch 140, Loss : 1.8418309442690939\n",
      "Epoch 150, Loss : 1.7415316270797163\n",
      "Epoch 160, Loss : 1.651043706429586\n",
      "Epoch 170, Loss : 1.5689918886200849\n",
      "Epoch 180, Loss : 1.4942520298712823\n",
      "Epoch 190, Loss : 1.4258950068674774\n",
      "Epoch 200, Loss : 1.363145222598574\n",
      "Epoch 210, Loss : 1.3053494549496696\n",
      "Epoch 220, Loss : 1.2519531340692656\n",
      "Epoch 230, Loss : 1.2024820377658674\n",
      "Epoch 240, Loss : 1.1565279954835508\n",
      "Epoch 250, Loss : 1.1137375980258895\n",
      "Epoch 260, Loss : 1.0738031893551407\n",
      "Epoch 270, Loss : 1.0364556112397392\n",
      "Epoch 280, Loss : 1.0014583088447282\n",
      "Epoch 290, Loss : 0.968602503616798\n",
      "Epoch 300, Loss : 0.9377032109979191\n",
      "Epoch 310, Loss : 0.9085959326777222\n",
      "Epoch 320, Loss : 0.8811338917654193\n",
      "Epoch 330, Loss : 0.8551857082240979\n",
      "Epoch 340, Loss : 0.8306334338161481\n",
      "Epoch 350, Loss : 0.8073708825322785\n",
      "Epoch 360, Loss : 0.7853022053565153\n",
      "Epoch 370, Loss : 0.7643406682215812\n",
      "Epoch 380, Loss : 0.7444075998370454\n",
      "Epoch 390, Loss : 0.7254314822443583\n",
      "Epoch 400, Loss : 0.7073471618527251\n",
      "Epoch 410, Loss : 0.6900951626254046\n",
      "Epoch 420, Loss : 0.6736210862345714\n",
      "Epoch 430, Loss : 0.6578750865494819\n",
      "Epoch 440, Loss : 0.6428114078938157\n",
      "Epoch 450, Loss : 0.6283879782014202\n",
      "Epoch 460, Loss : 0.614566049590912\n",
      "Epoch 470, Loss : 0.6013098800281648\n",
      "Epoch 480, Loss : 0.5885864506979949\n",
      "Epoch 490, Loss : 0.5763652144994331\n",
      "Epoch 500, Loss : 0.564617871741959\n",
      "Epoch 510, Loss : 0.5533181696765817\n",
      "Epoch 520, Loss : 0.542441722964372\n",
      "Epoch 530, Loss : 0.5319658525812888\n",
      "Epoch 540, Loss : 0.5218694409942004\n",
      "Epoch 550, Loss : 0.5121328017288777\n",
      "Epoch 560, Loss : 0.502737561694778\n",
      "Epoch 570, Loss : 0.49366655484023403\n",
      "Epoch 580, Loss : 0.48490372589092134\n",
      "Epoch 590, Loss : 0.47643404307871695\n",
      "Epoch 600, Loss : 0.46824341890115684\n",
      "Epoch 610, Loss : 0.4603186380668107\n",
      "Epoch 620, Loss : 0.4526472918816846\n",
      "Epoch 630, Loss : 0.4452177184185169\n",
      "Epoch 640, Loss : 0.43801894788632867\n",
      "Epoch 650, Loss : 0.4310406526835584\n",
      "Epoch 660, Loss : 0.42427310167572785\n",
      "Epoch 670, Loss : 0.41770711828913726\n",
      "Epoch 680, Loss : 0.41133404205644664\n",
      "Epoch 690, Loss : 0.405145693289011\n",
      "Epoch 700, Loss : 0.3991343405852518\n",
      "Epoch 710, Loss : 0.39329267091467923\n",
      "Epoch 720, Loss : 0.38761376204405784\n",
      "Epoch 730, Loss : 0.38209105709594926\n",
      "Epoch 740, Loss : 0.37671834105100976\n",
      "Epoch 750, Loss : 0.3714897190241191\n",
      "Epoch 760, Loss : 0.36639959616113943\n",
      "Epoch 770, Loss : 0.3614426590179399\n",
      "Epoch 780, Loss : 0.356613858296614\n",
      "Epoch 790, Loss : 0.35190839282565856\n",
      "Epoch 800, Loss : 0.3473216946815069\n",
      "Epoch 810, Loss : 0.34284941535832225\n",
      "Epoch 820, Loss : 0.3384874129014741\n",
      "Epoch 830, Loss : 0.33423173992781247\n",
      "Epoch 840, Loss : 0.33007863246272534\n",
      "Epoch 850, Loss : 0.3260244995301997\n",
      "Epoch 860, Loss : 0.32206591343769475\n",
      "Epoch 870, Loss : 0.3181996007026843\n",
      "Epoch 880, Loss : 0.31442243357231414\n",
      "Epoch 890, Loss : 0.31073142209173804\n",
      "Epoch 900, Loss : 0.3071237066804392\n",
      "Epoch 910, Loss : 0.30359655117924583\n",
      "Epoch 920, Loss : 0.3001473363338268\n",
      "Epoch 930, Loss : 0.2967735536832476\n",
      "Epoch 940, Loss : 0.29347279982470825\n",
      "Epoch 950, Loss : 0.2902427710279164\n",
      "Epoch 960, Loss : 0.28708125817463637\n",
      "Epoch 970, Loss : 0.28398614200089817\n",
      "Epoch 980, Loss : 0.28095538862109143\n",
      "Epoch 990, Loss : 0.27798704531478347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the labels (5x5 arrays)\n",
    "labels = np.array([\n",
    "    [\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1],\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Corresponding labels (0-5)\n",
    "targets = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "X = np.array([label.flatten() for label in labels])  # Flatten each label (5x5 -> 25x1)\n",
    "y = np.zeros((len(targets), 5))  # One-hot encode the targets\n",
    "for i, target in enumerate(targets):\n",
    "    y[i, target-1] = 1\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_pred=layers.forward(X)\n",
    "\n",
    "    loss_value = loss.forward(y_pred, y)\n",
    "\n",
    "    grad_out=loss.backward()\n",
    "    layers.backward(grad_out)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss : {loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98344941, 0.00505547, 0.0044613 , 0.00202382, 0.00501   ],\n",
       "       [0.001903  , 0.93920295, 0.04803932, 0.00209331, 0.00876141],\n",
       "       [0.00293025, 0.0472595 , 0.89421591, 0.00805075, 0.0475436 ],\n",
       "       [0.00108336, 0.00217839, 0.01526053, 0.97929427, 0.00218346],\n",
       "       [0.00188237, 0.00882746, 0.04825068, 0.0020353 , 0.93900419]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class : 2\n"
     ]
    }
   ],
   "source": [
    "test_input = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "test_input_flattened = test_input.flatten().reshape(1,-1)\n",
    "\n",
    "predicted_output = layers.forward(test_input_flattened)\n",
    "\n",
    "predicted_class = np.argmax(predicted_output, axis=1) + 1\n",
    "\n",
    "print(f\"Predicted Class : {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_classes = np.argmax(layers.forward(X), axis=1) + 1  # 1-based 클래스 예측\n",
    "accuracy = calculate_accuracy(targets, y_pred_classes)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ohdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
